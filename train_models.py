import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import pickle
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import os

# ==============================
# 0. PATH & Táº O FOLDER LÆ¯U áº¢NH
# ==============================

DATA_PATH = Path("data") / "houses.csv"
MODELS_DIR = Path("models")
MODELS_DIR.mkdir(exist_ok=True)

IMG_DIR = Path("static") / "img"
os.makedirs(IMG_DIR, exist_ok=True)

# ==============================
# 1. Äá»ŒC & CHUáº¨N Bá»Š Dá»® LIá»†U
# ==============================

df = pd.read_csv(DATA_PATH)

# TÃCH ZIPCODE Tá»ª statezip (vd: "WA 98136" -> 98136)
df["zipcode"] = (
    df["statezip"]
    .astype(str)
    .str.extract(r"(\d+)", expand=False)
)
df["zipcode"] = pd.to_numeric(df["zipcode"], errors="coerce")

# Bá»˜ THUá»˜C TÃNH ÄANG DÃ™NG
feature_cols = [
    "bedrooms",        # sá»‘ phÃ²ng ngá»§
    "bathrooms",       # sá»‘ phÃ²ng táº¯m
    "sqft_living",     # diá»‡n tÃ­ch sinh hoáº¡t
    "sqft_lot",        # diá»‡n tÃ­ch Ä‘áº¥t
    "floors",          # sá»‘ táº§ng
    "waterfront",      # cÃ³ view máº·t nÆ°á»›c khÃ´ng (0/1)
    "view",            # cháº¥t lÆ°á»£ng view (0-4)
    "condition",       # tÃ¬nh tráº¡ng nhÃ  (1-5)
    "sqft_above",      # diá»‡n tÃ­ch trÃªn máº·t Ä‘áº¥t
    "sqft_basement",   # diá»‡n tÃ­ch táº§ng háº§m
    "yr_built",        # nÄƒm xÃ¢y
    "yr_renovated",    # nÄƒm cáº£i táº¡o
    "zipcode",         # mÃ£ vÃ¹ng
    "city",            # thÃ nh phá»‘
]

target_col = "price"   # giÃ¡ nhÃ  (USD) - biáº¿n má»¥c tiÃªu

df = df[feature_cols + [target_col]].copy()

numeric_cols = [
    "bedrooms", "bathrooms", "sqft_living", "sqft_lot",
    "floors", "waterfront", "view", "condition",
    "sqft_above", "sqft_basement", "yr_built",
    "yr_renovated", "zipcode",
]

for col in numeric_cols:
    df[col] = pd.to_numeric(df[col], errors="coerce")

df = df.dropna(subset=feature_cols + [target_col])

X = df[feature_cols]
y = df[target_col]

# ==============================
# 2. TIá»€N Xá»¬ LÃ
# ==============================

numeric_features = numeric_cols
categorical_features = ["city"]

preprocess = ColumnTransformer(
    transformers=[
        ("num", "passthrough", numeric_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
    ]
)

# ==============================
# 3. MÃ” HÃŒNH RANDOM FOREST
# ==============================

rf_regressor = RandomForestRegressor(
    n_estimators=500,
    max_depth=40,
    min_samples_split=2,
    min_samples_leaf=1,
    random_state=42,
    n_jobs=-1
)

model = Pipeline(steps=[
    ("preprocess", preprocess),
    ("rf", rf_regressor),
])

# ==============================
# 4. CHIA TRAIN / TEST
# ==============================

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ==============================
# 5. TRAIN & ÄÃNH GIÃ
# ==============================

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred) ** 0.5

print("=== Random Forest Regression (tá»‘i Æ°u vá»›i zipcode) ===")
print(f"R2   : {r2:.4f}")
print(f"MAE  : {mae:,.2f}")
print(f"RMSE : {rmse:,.2f}")

# ==============================
# 6. LÆ¯U MODEL
# ==============================

model_path = MODELS_DIR / "house_price_rf.pkl"
with open(model_path, "wb") as f:
    pickle.dump(model, f)

print(f"\nâœ… ÄÃ£ lÆ°u model Random Forest táº¡i: {model_path}")

# ==============================
# 7. Váº¼ CÃC BIá»‚U Äá»’ Äá»‚ VIáº¾T BÃO CÃO
# ==============================

# ----- 7.1. Scatter: GiÃ¡ thá»±c táº¿ vs. GiÃ¡ dá»± Ä‘oÃ¡n -----
plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred, alpha=0.3)
min_price = min(y_test.min(), y_pred.min())
max_price = max(y_test.max(), y_pred.max())
plt.plot([min_price, max_price], [min_price, max_price], "--")
plt.xlabel("GiÃ¡ thá»±c táº¿ (USD)")
plt.ylabel("GiÃ¡ dá»± Ä‘oÃ¡n (USD)")
plt.title("So sÃ¡nh giÃ¡ thá»±c táº¿ vs giÃ¡ dá»± Ä‘oÃ¡n")
plt.tight_layout()
scatter_path = IMG_DIR / "y_true_vs_pred.png"
plt.savefig(scatter_path, dpi=150)
plt.close()
print(f"ğŸ“Š ÄÃ£ lÆ°u biá»ƒu Ä‘á»“: {scatter_path}")

# ----- 7.2. Histogram: PhÃ¢n bá»‘ sai sá»‘ (residuals) -----
residuals = y_test - y_pred
plt.figure(figsize=(6, 4))
plt.hist(residuals, bins=50)
plt.xlabel("Sai sá»‘ (GiÃ¡ thá»±c táº¿ - GiÃ¡ dá»± Ä‘oÃ¡n) [USD]")
plt.ylabel("Táº§n suáº¥t")
plt.title("PhÃ¢n bá»‘ sai sá»‘ dá»± Ä‘oÃ¡n (Residuals)")
plt.tight_layout()
residuals_path = IMG_DIR / "residuals_hist.png"
plt.savefig(residuals_path, dpi=150)
plt.close()
print(f"ğŸ“Š ÄÃ£ lÆ°u biá»ƒu Ä‘á»“: {residuals_path}")

# ----- 7.3. Feature Importance (tá»•ng há»£p theo feature gá»‘c) -----
rf = model.named_steps["rf"]
pre = model.named_steps["preprocess"]

# tÃªn cá»™t numeric giá»¯ nguyÃªn
num_names = np.array(numeric_features)

# tÃªn cá»™t sau one-hot cho city
cat_transformer = pre.named_transformers_["cat"]
city_feature_names = cat_transformer.get_feature_names_out(["city"])

all_feature_names = np.concatenate([num_names, city_feature_names])
importances = rf.feature_importances_

# Láº¥y top 15 feature quan trá»ng nháº¥t
idx = np.argsort(importances)[::-1][:15]
top_features = all_feature_names[idx]
top_importances = importances[idx]

plt.figure(figsize=(8, 5))
plt.barh(range(len(top_features)), top_importances[::-1])
plt.yticks(range(len(top_features)), top_features[::-1])
plt.xlabel("Äá»™ quan trá»ng tÆ°Æ¡ng Ä‘á»‘i")
plt.title("Top 15 thuá»™c tÃ­nh quan trá»ng nháº¥t (Random Forest)")
plt.tight_layout()
fi_path = IMG_DIR / "feature_importance.png"
plt.savefig(fi_path, dpi=150)
plt.close()
print(f"ğŸ“Š ÄÃ£ lÆ°u biá»ƒu Ä‘á»“: {fi_path}")
